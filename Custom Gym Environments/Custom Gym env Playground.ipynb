{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's learn how custom environments are built with OpenAI's GYM\n",
    "\n",
    "## Why am I learning to make environments in gym\n",
    "\n",
    "I'm following gym's official tutorial for this, I'll try to summarize the stuff here, but this is more of a playground where i just test stuff without worrying about it, its included in the repo anyway becuz it kinda makes sense, im learning it to make the maze environment to test how dynaQ+ learns to beat dynaQ on changing the changing environment... I could do it simply but most of my controls are writen to expect gym Env objects... plus its useful to have some software framework to follow when making environments, afterall in the real world you'll be making your own environments for most of the software based environments...i.e custom games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pygame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The constructor\n",
    "- Specify Observation space (use the spaces module for observation_space and action space)\n",
    "- Specify Action space\n",
    "- Specify Action mappings (int->action) e.g 0->right, 1->up ...\n",
    "- Manage rendering modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Observations\n",
    "- Return objects that the agent will get upon calling env.reset or env.step\n",
    "- It can be implemented separately or as a common function like here in _get_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset and Step\n",
    "- Gym really gives you the freedom of doing whatever you want with reset and step as it should\n",
    "- In reset, reset the states as you wish, in step transform the states as you wish ðŸ™„\n",
    "- step returns (next_state, reward, done, truncated(optional) , info\n",
    "- You may have never encountered truncated before like me, but it basically means whether the observation is complete or not... like in rgb observations you may reduce the original observation to be able to manage it becuz its a lot of data... so you'll return truncated to be true, here it is false, because what we observe is what we give to the agent back in form of observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\":4}\n",
    "    def __init__(self, render_mode=None, size=5):\n",
    "        self.size = size\n",
    "        self.window_size = 512 # pygame window size\n",
    "        \n",
    "# agent and target locations are encoded within the observation space using spaces.dict (specific to this env)\n",
    "# the observation space should be defined by spaces... there is no other way\n",
    "# this is essential and it really is a painless process\n",
    "\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"agent\": spaces.Box(0, size-1, shape=(2,), dtype=int),\n",
    "            \"target\": spaces.Box(0, size-1, shape=(2,), dtype=int)\n",
    "        })\n",
    "        \n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.nS = 2* (size**2)\n",
    "        self.nA = 4\n",
    "#         dictionary map (a: (x, y))\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1,0]),\n",
    "            1: np.array([0,1]),\n",
    "            2: np.array([-1,0]),\n",
    "            3: np.array([0,-1]),\n",
    "        }\n",
    "        \n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "    \n",
    "    def _get_obs(self):\n",
    "# we need to convert the environment into an observation\n",
    "# you can write a common method for both reset and step like this or do it separately\n",
    "        return {\"agent\": self._agent_location, \"target\":self._target_location}\n",
    "    \n",
    "    def _get_info(self):\n",
    "#         just as some extra piece of information we can provide manhattan distance\n",
    "# its really just a distance in terms of boxes/units (DX (boxes difference along X)+ (DY(boxes difference along Y)))\n",
    "# ord = 1 means its calculating manhattan distance... which is really just sum of the absolutes of the difference in this case\n",
    "\n",
    "        return {\"distance\": np.linalg.norm(self._agent_location - self._target_location, ord=1)}\n",
    "    \n",
    "    def reset(self, seed = None, options = None):\n",
    "#         we also need to worry about PRNGs here... gym takes care of that for you.\n",
    "# allow the function to pass seed and make sure to do super().reset(seed = seed)\n",
    "#         super().reset()\n",
    "    \n",
    "        self._agent_location = np.random.randint(0, self.size, size=(2,), dtype=int)\n",
    "        self._target_location = self._agent_location\n",
    "        \n",
    "        while np.array_equal(self._agent_location, self._target_location):\n",
    "            self._target_location = np.random.randint(0, self.size, size=(2,), dtype=int)\n",
    "            \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "            \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        direction = self._action_to_direction[action]\n",
    "        \n",
    "        self._agent_location = np.clip(\n",
    "        self._agent_location + direction, 0, self.size-1)\n",
    "        \n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "        \n",
    "#         rewards are sparse (u get 1 when u reach target else u get 0), its called binary sparse rewards lol\n",
    "\n",
    "        reward = 1 if terminated else 0\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "        \n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        \n",
    "        return observation, reward, terminated,False, info\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode ==\"rgb_array\":\n",
    "            return self._render_frame()\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode((self.window_size, self.window_size))\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "        # Now we draw the agent\n",
    "        pygame.draw.circle(\n",
    "            canvas,\n",
    "            (0, 0, 255),\n",
    "            (self._agent_location + 0.5) * pix_square_size,\n",
    "            pix_square_size / 3,\n",
    "        )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(nS, nA):\n",
    "#     0,0,0,0 -> 4,4,4,4\n",
    "    indices = np.indices((5,5,5,5))\n",
    "    keys = np.stack(indices, axis=-1).reshape((-1,4))\n",
    "    Q = {tuple(key): np.random.normal(0.5, 0.25, (nA,)) for key in keys}\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QControl:\n",
    "    def __init__(self, nS, nA, terminal=list(range(37,48)), eps=None, initializer=None):\n",
    "        self.Q = np.random.normal(0.5,0.25, (nS, nA) )\n",
    "\n",
    "        \n",
    "        if initializer!=None:\n",
    "            self.Q = initializer(nS, nA)\n",
    "        \n",
    "        \n",
    "        for i in terminal:\n",
    "            self.Q[i] = np.array([0, 0, 0, 0])\n",
    "        \n",
    "        self.pi = np.random.randint(0, 4, (16, ), dtype=np.int8)\n",
    "        self.alpha = 0.2\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = eps\n",
    "        self.last_action = 0\n",
    "    \n",
    "    def policy(self, state, eps=None):\n",
    "        \n",
    "        if eps == None:\n",
    "            if self.epsilon != None:\n",
    "                eps = self.epsilon\n",
    "            else:\n",
    "                eps = 0.2\n",
    "        \n",
    "        \n",
    "        if np.random.random() >= eps:\n",
    "            return np.argmax(self.Q[state])\n",
    "        else:\n",
    "            return np.random.randint(0, 4)\n",
    "        \n",
    "    def learn(self, s, env, step_size, eps=None):\n",
    "        if eps == None:\n",
    "            if self.epsilon != None:\n",
    "                eps = self.epsilon\n",
    "            else:\n",
    "                eps = 0.2\n",
    "        a = self.policy(s, eps=eps)\n",
    "        s_, r, done, _, info = env.step(a)\n",
    "        \n",
    "        s_ = tuple([*s_[\"agent\"], *s_[\"target\"] ])\n",
    "        self.Q[s][a] = self.Q[s][a] + step_size*(r + np.max(self.Q[s_]) - self.Q[s][a]   )\n",
    "        \n",
    "        self.last_action = a\n",
    "        \n",
    "        return s_, r, done,_, info\n",
    "    \n",
    "    def plan(self, s, model, step_size,a,  eps=None):\n",
    "        \n",
    "                \n",
    "        r, s_ = model[(s,a)]\n",
    "        self.Q[s][a] = self.Q[s][a] + step_size*(r + np.max(self.Q[s_]) - self.Q[s][a]   )\n",
    "        self.last_action = a\n",
    "        return s_, r, False, {}\n",
    "    \n",
    "    def load_q_values(self, file_location):\n",
    "        f = open(file_location, \"rb\")\n",
    "        self.Q = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dyna(QControl, env, planning_steps, n_episodes,  step_size=0.5, episode_limit = 500):\n",
    "    \n",
    "    q_control = QControl(env.nS, env.nA, eps=0.2, terminal=[], initializer=initializer)\n",
    "    model = {}\n",
    "    \n",
    "    observed_states = set({})\n",
    "    \n",
    "    indices = np.indices((5,5,5,5))\n",
    "    keys = np.stack(indices, axis=-1).reshape((-1,4))\n",
    "    \n",
    "    observed_actions = {tuple(key):set() for key in keys}\n",
    "    curr_s = env.reset()\n",
    "    rewards = []\n",
    "    Rs= 0\n",
    "    steps = []\n",
    "    for i in range(n_episodes):\n",
    "        if i%100 == 0:\n",
    "            print(\"{:.2f} % done\".format(i/n_episodes))\n",
    "        done = False\n",
    "        Rs = 0\n",
    "        steps_this_episode = 0\n",
    "        curr_s = env.reset()\n",
    "        curr_s = tuple([*curr_s[0][\"agent\"], *curr_s[0][\"target\"] ])\n",
    "        while not done and steps_this_episode < episode_limit:\n",
    "        \n",
    "            s_, r, done, _,info = q_control.learn(curr_s, env, step_size)\n",
    "            a = q_control.last_action\n",
    "            \n",
    "            model[curr_s, a] = [r, s_]\n",
    "\n",
    "\n",
    "            observed_actions[curr_s].add(a)\n",
    "            observed_states.add(curr_s)\n",
    "            curr_s = s_\n",
    "            Rs+=r\n",
    "            steps_this_episode += 1\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            for n in range(planning_steps):\n",
    "                states = list(observed_states)\n",
    "                \n",
    "                s = np.random.choice(np.arange(len(states)))\n",
    "                actions = list(observed_actions[states[s]])\n",
    "                \n",
    "                a = np.random.choice(actions)\n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "                q_control.plan(states[s], model, step_size, a)\n",
    "\n",
    "        rewards.append(Rs)\n",
    "        steps.append(steps_this_episode)\n",
    "    return rewards, steps, q_control\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 % done\n",
      "0.05 % done\n",
      "0.10 % done\n",
      "0.15 % done\n",
      "0.20 % done\n",
      "0.25 % done\n",
      "0.30 % done\n",
      "0.35 % done\n",
      "0.40 % done\n",
      "0.45 % done\n",
      "0.50 % done\n",
      "0.55 % done\n",
      "0.60 % done\n",
      "0.65 % done\n",
      "0.70 % done\n",
      "0.75 % done\n",
      "0.80 % done\n",
      "0.85 % done\n",
      "0.90 % done\n",
      "0.95 % done\n"
     ]
    }
   ],
   "source": [
    "rs0, steps0, qcontrol = dyna(QControl, env, 30, 2000, step_size=0.01, episode_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d36e627460>]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(steps0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model ={((4, 0, 4, 2), 0): [0, (4, 0, 4, 2)]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, (4, 0, 4, 2)]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[((4, 0, 4, 2), 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_policy(control, limit=30):\n",
    "    env.reset()\n",
    "#     out = env.render(mode=\"ansi\")\n",
    "    done = False\n",
    "    s = env.reset()\n",
    "    s = tuple([*s[0][\"agent\"], *s[0][\"target\"] ])\n",
    "    i = 0\n",
    "    while not done:\n",
    "        \n",
    "        s_, r, done, prop, _ = env.step(np.argmax(control.Q[s]))\n",
    "        \n",
    "        env.render()\n",
    "        s_ = tuple([*s_[\"agent\"], *s_[\"target\"] ])\n",
    "        s = s_\n",
    "        i+=1\n",
    "        if done:\n",
    "            print(\"congrats\")\n",
    "        if i >= limit:\n",
    "            done = True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "control = QControl(env.nS, env.nA, eps=0.2, terminal=[], initializer=initializer)\n",
    "control.load_q_values(\"Q_Values_Grid_World.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "congrats\n"
     ]
    }
   ],
   "source": [
    "demo_policy(control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_q_values(Q):\n",
    "    with open(\"Q_Values_Grid_World.pkl\", \"wb\") as f:\n",
    "        pickle.dump(Q, f)\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_q_values(qcontrol.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "!explorer .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
